{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubedSphereCommunicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Some features of this notebook such as gather may require moving it to the `notebooks/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a little different from the others, because we're going to run it using MPI through ipyparallel. The `ipcluster` command will start a cluster of MPI processes. We will use `%autopx` to enable a parallel mode where the code you run in each cell runs on all ranks in the cluster, instead of being run on the notebook process (which is not in the cluster). At the end, we will use `%autopx` again to switch back to the notebook process and shut down the cluster.\n",
    "\n",
    "It's possible that while running this notebook, you might encounter a deadlock or get into an otherwise inconsistent state. If that happens, go to the bottom two cells of this notebook starting with `%autopx` and run them, then start the notebook again from the top. If this doesn't work, you should shut down this notebook, use the console to kill the `mpiexec` process, and then restart the notebook. Alternatively you can cancel the Orion job hosting the notebook and re-submit a new job with a new notebook server instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So without further ado, let's start the ipcluster. Make sure the workshop directory corresponds to where you've checked out the workshop repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"${DOCKER}\" != \"True\" ] ; then\n",
    "    source ~/workshop/venv/bin/activate\n",
    "fi\n",
    "ipcluster start --profile=mpi -n 6 --daemonize\n",
    "sleep 10  # command is asynchronous, so let's wait to avoid an error in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client(profile='mpi', targets='all', block=True)\n",
    "dv = rc[:]\n",
    "dv.activate()\n",
    "dv.block = True\n",
    "print(\"Running IPython Parallel on {0} MPI engines\".format(len(rc.ids)))\n",
    "print(\"Commands in the following cells will be executed in parallel (disable with %autopx)\")\n",
    "%autopx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above should say `%autopx enabled`. If it says `%autopx disabled`, run a new cell with only `%autopx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm MPI is working and running on 6 ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "print(f\"Number of ranks is {mpi_size}.\")\n",
    "print(f\"I am rank {mpi_rank}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to import the symbols we're going to use from `fv3gfs.util`, as well as other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fv3gfs.util import (\n",
    "    TilePartitioner, CubedSpherePartitioner, CubedSphereCommunicator, Quantity,\n",
    "    X_DIM, Y_DIM, Z_DIM, X_INTERFACE_DIM, Y_INTERFACE_DIM\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "# we will add scatter and gather to CubedSphereCommunicator in the future,\n",
    "# for now this function is a limited implementation for this workshop.\n",
    "# This lets us do some plotting.\n",
    "from tools import gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll initialize a partitioner and communicator for a 1-by-1 tile layout (i.e. one rank for each tile face). We're using a reduced layout because it is easier to read the output from 6 ranks than 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful changing this, we only initialized 6 ranks!\n",
    "# if you do explore different layouts, start a new ipcluster\n",
    "# layouts bigger than (2, 2) may not work (too many processes on one node)\n",
    "layout = (1, 1)\n",
    "cube = CubedSphereCommunicator(\n",
    "    MPI.COMM_WORLD,\n",
    "    CubedSpherePartitioner(\n",
    "        TilePartitioner(layout)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this session has been to get to the point where we can do a halo update. So let's finally do one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by initializing a quantity on each rank filled with the rank's value. When we halo update it, we can see which halos are updated from which nearby ranks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity = Quantity(\n",
    "    np.full((6, 6), cube.rank, dtype=np.float64),\n",
    "    origin=(1, 1),\n",
    "    extent=(4, 4),\n",
    "    dims=[X_DIM, Y_DIM],\n",
    "    units=\"\",\n",
    "    gt4py_backend=\"numpy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's plot the initial state of each quantity, using the same colorbar on each rank. This should show each rank's data is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# transpose is needed so the plot shows the first axis as the x axis\n",
    "plt.pcolormesh(quantity.data[:].T, vmin=0, vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a halo update! This part is up to you. Remember that in the allocation above, we only gave the quantity one halo point. When you plot the data afterwards, you should see the halos filled with data from adjacent tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer: cube.halo_update(quantity, n_points=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# transpose is needed so the plot shows the first axis as the x axis\n",
    "plt.pcolormesh(quantity.data[:].T, vmin=0, vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see (hopefully) that the corner data is unchanged (and that the rest of the halo is updated). When we're using one rank per tile face, why do we expect this to be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot all the data from the first rank, so we can better see the relationship betwen ranks. First, we need to gather all the data onto the root rank. Notice that the other ranks do not receive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_quantity = gather(quantity, cube)\n",
    "print(global_quantity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data on one rank, you could use any approach you like to plot it on the globe. Cartopy projections can work well for this. Since Cartopy is hard to install, we've implemented a simple \"flattened cube\" projection below.\n",
    "\n",
    "To plot on a cube with pcolormesh, each tile's 2D data needs to be plotted in a separate command. This also requires a value for vmin and vmax, so that the same colormap bounds are used on each process!\n",
    "\n",
    "First, we'll generate the x and y positions on the plot for each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(shape):\n",
    "    X = np.zeros([shape[0], shape[1] + 1, shape[2] + 1]) + np.arange(0, shape[1] + 1)[None, :, None]\n",
    "    Y = np.zeros([shape[0], shape[1] + 1, shape[2] + 1]) + np.arange(0, shape[2] + 1)[None, None, :]\n",
    "    # offset and rotate the data for each rank, with zero at the \"center\"\n",
    "    for tile, shift_x, shift_y, n_rotations in [\n",
    "        (1, 1, 0, 0), (2, 0, 1, -1), (3, 2, 0, 1), (4, -1, 0, 1), (5, 0, -1, 0)\n",
    "    ]:\n",
    "        X[tile, :, :] += shift_x * shape[1]\n",
    "        Y[tile, :, :] += shift_y * shape[2]\n",
    "        X[tile, :, :] = np.rot90(X[tile, :, :], n_rotations)\n",
    "        Y[tile, :, :] = np.rot90(Y[tile, :, :], n_rotations)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_quantity is not None:  # only true on the first rank\n",
    "    X, Y = get_X_Y(global_quantity.extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use these positions with pcolormesh to plot the cube. Including an `if` statement ensures the plotting only happens on the first rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_quantity is not None:\n",
    "    plt.figure(figsize=(9, 5.5))\n",
    "    for tile in range(global_quantity.extent[0]):\n",
    "        im = plt.pcolormesh(X[tile, :, :], Y[tile, :, :], global_quantity.view[tile, :, :], vmin=0, vmax=5)\n",
    "    plt.colorbar(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again at the cube image, keeping in mind rank 0 is on tile 1 (we numbered the tiles based on FV3GFS's restart file indices). Do the positions of the ranks appear as expected in your plots above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/cube.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a wrapped function for the plotting above to be re-used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_global(quantity, vmin, vmax):\n",
    "    # gather is written for this workshop to collapse any third\n",
    "    # dimension (e.g. vertical) by selecting only the first index\n",
    "    global_quantity = gather(quantity, cube)\n",
    "    if global_quantity is not None:  # only on first rank\n",
    "        X, Y = get_X_Y(global_quantity.extent)\n",
    "        plt.figure(figsize=(9, 5.5))\n",
    "        for tile in range(global_quantity.extent[0]):\n",
    "            im = plt.pcolormesh(\n",
    "                X[tile, :, :],\n",
    "                Y[tile, :, :],\n",
    "                global_quantity.view[tile, :, :],\n",
    "                vmin=vmin,\n",
    "                vmax=vmax,\n",
    "        )\n",
    "        plt.colorbar(im)\n",
    "        # we don't plt.show() here in case you want to run more commands after plot_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can perform halo updates, we'll look at some more interesting data. Here we provide a routine to load latitude and longitude data from a C48 data file. You can use sizes that evenly divide 48, like 6, 8, or 12.\n",
    "\n",
    "These routines run in each process and load only the data needed for the current rank, which is the same as a tile for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = xr.open_zarr(\"data/c48.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_lat_lon_bounds(nx: int, ny: int, tile: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return latitude and longitude on the corners of grid cells.\n",
    "    \n",
    "    Takes in the number of grid cells and the tile index.\n",
    "    \"\"\"\n",
    "    assert 48 % nx == 0, \"48 must be divisible by nx\"\n",
    "    assert 48 % ny == 0, \"48 must be divisible by ny\"\n",
    "    lat = np.empty([nx+1, ny+1])\n",
    "    lon = np.empty([nx+1, ny+1])\n",
    "    for ix_new, ix_orig in enumerate(np.linspace(0, 48, nx+1, dtype=int)):\n",
    "        for iy_new, iy_orig in enumerate(np.linspace(0, 48, ny+1, dtype=int)):\n",
    "            lat[ix_new, iy_new] = grid[\"latb\"][tile, iy_orig, ix_orig]\n",
    "            lon[ix_new, iy_new] = grid[\"lonb\"][tile, iy_orig, ix_orig]\n",
    "    return lat, lon\n",
    "\n",
    "def mean_of_corners(array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Given values on cell corners, return the average for each cell.\n",
    "    \n",
    "    Assumes input arrays have dimensions [x, y] or [y, x].\n",
    "    \"\"\"\n",
    "    return 0.25 * (array[1:, 1:] + array[:-1, 1:] + array[1:, :-1] + array[:-1, :-1])\n",
    "\n",
    "def get_lat_lon(nx: int, ny: int, cube: CubedSphereCommunicator) -> Tuple[Quantity, Quantity]:\n",
    "    lat_bounds, lon_bounds = get_lat_lon_bounds(nx, ny, cube.rank)\n",
    "    lat_array = mean_of_corners(lat_bounds)  # not strictly accurate, but good enough\n",
    "    lon_array = mean_of_corners(lon_bounds)\n",
    "    lat = Quantity(  # no halo, so we don't need origin and extent\n",
    "        lat_array,\n",
    "        dims=[X_DIM, Y_DIM],\n",
    "        units=\"degrees_east\",\n",
    "        gt4py_backend=\"numpy\",\n",
    "    )\n",
    "    lon = Quantity(\n",
    "        lon_array,\n",
    "        dims=[X_DIM, Y_DIM],\n",
    "        units=\"degrees_north\",\n",
    "        gt4py_backend=\"numpy\",\n",
    "    )\n",
    "    return lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the latitude and longitude look like in our flattened cube plot! Keep in mind you need to update X and Y for plotting whenever your data changes shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = ny = 8\n",
    "lat, lon = get_lat_lon(nx, ny, cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_global(lat, vmin=-90, vmax=90)\n",
    "plot_global(lon, vmin=0, vmax=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: We invite you to compute your own favorite function of latitude and longitude on each rank, gather it onto a single rank, and plot it! For example, for a Gaussian centered on GFDL, you could try `gaussian = np.exp(- ((lat.view[:] - 40.35)**2 + (lon.view[:] - 74.67)**2)/(2*30**2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "gaussian = Quantity(\n",
    "    np.exp(- ((lat.view[:] - 40.35)**2 + (lon.view[:] - 74.67)**2)/(2*30**2)),\n",
    "    dims=[X_DIM, Y_DIM],\n",
    "    units=\"\"\n",
    ")\n",
    "\n",
    "plot_global(gaussian, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done so far is the tip of the iceberg. Now we have all the tools needed to run parallel GT4py code on a cubed sphere with halo updates. Before we start, let's import gt4py and run a command needed to prevent hangs from filesystem collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gt4py import gtscript\n",
    "import gt4py\n",
    "\n",
    "# need to use a different gt cache on each rank, for now\n",
    "# without this, your notebook and ipcluster will hang and need to be restarted\n",
    "gt4py.config.cache_settings[\"dir_name\"] = f\".gt_cache_{cube.rank:0>4d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use what you learned in the rest of the workshop to implement a routine of your choice. You can copy a stencil from earlier in this workshop such as diffusion (e.g. `laplacian` in notebook Session-A1.2) or hyperdiffusion, or you can implement a new routine altogether.\n",
    "\n",
    "Some points to keep in mind in this section:\n",
    "\n",
    "- we have shown mainly 2D quantities so far, but gt4py stencils currently require 3D inputs. We suggest using a `[X_DIM, Y_DIM, Z_DIM]` quantity with a length of 1 on the z-dimension. While Quantity supports arbitrary dimension ordering, gt4py does not yet do so (coming soon).\n",
    "- if you implement a truly 3D stencil with more than 1 vertical level, the gather routine we have written for this workshop and are using for plotting will gather only the lowest vertical level.\n",
    "- you can call multiple stencils in sequence if you wish, just remember to halo update as needed\n",
    "- ensure you use the same backend and dtype for the stencil and Quantity\n",
    "- The starter code after this cell will assume you write a stencil called `update` which takes in one input and updates it to the next timestep, you are free to change this and update the later cells.\n",
    "- For the purposes of this exercise, you can make the approximation that the gridcell width is constant, even though it's not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "backend = \"numpy\"\n",
    "dtype=np.float64\n",
    "\n",
    "@gtscript.function\n",
    "def d2x(u):\n",
    "    return (-2 * u[0, 0, 0] + u[-1, 0, 0] + u[1, 0, 0])\n",
    "\n",
    "@gtscript.function\n",
    "def d2y(u):\n",
    "    return (-2 * u[0, 0, 0] + u[0, -1, 0] + u[0, 1, 0])\n",
    "\n",
    "@gtscript.function\n",
    "def lap_cube_cells(u):\n",
    "    result = d2x(u) + d2y(u)\n",
    "    return result\n",
    "\n",
    "@gtscript.stencil(backend=backend)\n",
    "def diffuse(u: gtscript.Field[dtype], coeff: float):\n",
    "    with computation(PARALLEL), interval(...):\n",
    "        du = coeff * lap_cube_cells(u)\n",
    "        u = u + du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code to initialize a uniform random Quantity\n",
    "N = 12\n",
    "dx = 1.0 / (N-1)\n",
    "nhalo = 2\n",
    "shape = (N + 2 * nhalo, N + 2 * nhalo, 1)\n",
    "\n",
    "random = np.random.RandomState(cube.rank)\n",
    "# you can keep these separately to reset to the start values\n",
    "# with quantity.data[:] = start_values\n",
    "start_values = random.uniform(0, 1, size=shape).astype(dtype)\n",
    "quantity = Quantity(\n",
    "    start_values.copy(),\n",
    "    origin=(nhalo, nhalo, 0),\n",
    "    extent=(N, N, 1),\n",
    "    dims=[X_DIM, Y_DIM, Z_DIM],\n",
    "    units=\"\",\n",
    "    gt4py_backend=backend,\n",
    ")\n",
    "plot_global(quantity, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# reset to initial values for easy re-execution of this cell\n",
    "quantity.data[:] = start_values\n",
    "\n",
    "n_steps = 10\n",
    "for i in range(n_steps):\n",
    "    # halos need to be updated when we start the first timestep\n",
    "    cube.halo_update(quantity, n_points=1)\n",
    "    diffuse(quantity.storage, coeff=0.1)\n",
    "    plot_global(quantity, vmin=0, vmax=1)\n",
    "    if cube.rank == 0:\n",
    "        plt.title(f\"timestep {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice sharp boundaries at tile edges, make sure that you are halo updating at the right time. If you notice issues at corners, think hard about any special handling the corners need and whether they've been implemented correctly with regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autopx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.shutdown(hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
